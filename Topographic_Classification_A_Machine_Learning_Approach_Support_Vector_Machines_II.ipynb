{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Topographic Classification: a Machine Learning Approach using Support Vector Machines, Noteboook II</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we continue experiments with Support Vector Machine based models for identifying and mapping topographic landscapes. We continue with the same Eastern U.S. geography that was at the center of prior study. \n",
    "\n",
    "Preliminary results in applying the SVM to differentiate and classify the coastal plain from the upland are encouraging with classification accuracies exceeding 90%. \n",
    "\n",
    "The overarching assumption here continues to be that the coastal plain province, and by extension coastal plains in general, present topographic characteristics that differ geomorphically/geomorphometrically with the adjacent upland. Further, these differientiating characteristics can measured and used to build  classification model that distinguishes in some meaningful way, present a less topographically varied (i.e. rough) character than do the interior uplands [onto which the coastal plains abutt). Further, we assume that, in the presence of true assumptions, we can further identify, quantitatively model, and go on to map this difference and the two provinces--reliably. Repeatably. Maybe even globally!\n",
    "\n",
    "**NOTE2 THESE DATA ARE _NOT_ SLOPE CONSTRAINED: IN THIS NOTEBOOK WE ARE USING ALL SLOPES RANGING FROM 0 TO (up to) 90 DEGREES (if they exist)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Requisite Modules, Libraries, and Magics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score   # not using\n",
    "from sklearn.model_selection import KFold             # not using\n",
    "from sklearn.model_selection import validation_curve  # not using\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline            # not using\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import seaborn as sns\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the [labeled] sample observation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9787,) (9787, 3)\n"
     ]
    }
   ],
   "source": [
    "datapath='/Users/paulp/GoogleDrive/projects/GRASSData/USAtl_CoastalPlain_Prototype_LCC/user/sqlite/'\n",
    "file='sqlite.db'\n",
    "\n",
    "conn=sqlite3.connect(datapath+file)   # create a connection to the GRASS database file:\n",
    "\n",
    "# load the contents of the vector point file: train_test_10k_pts into a pandas dataframe\n",
    "# note that we are loading only those records whose elevation (z) is > 0 meters.\n",
    "# note that we are also dropping all records that contain one or more missing (NaN) values.\n",
    "df = pd.read_sql_query('select * from train_test_10k_pts where z > 0.0', conn).dropna()\n",
    "\n",
    "# build the standardized X features and y target arrays:\n",
    "scaler = StandardScaler()\n",
    "X=scaler.fit_transform(np.array(df[['z','slope','distance']]) )\n",
    "y=np.array(df['label']) \n",
    "print(np.shape(y), np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()\n",
    "#df.isnull().values.any()\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the Sample Data--Premodeling (Elevation, Slope, Distance to Shore)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plots for Elevation vs Slope\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(14,7)) \n",
    "\n",
    "ax1.scatter(X[:,0],X[:,1], c=y, s=14, cmap=mpl.cm.Paired)\n",
    "ax1.set_xlabel('Elevation (m)')\n",
    "ax1.set_ylabel('Slope (degrees)')\n",
    "ax1.set_title('Elevation vs. Slope')\n",
    "\n",
    "ax2.scatter(X[:,0],X[:,1], c=y, s=14, cmap=mpl.cm.Paired)\n",
    "ax2.set_xlim(-1.25, 0)\n",
    "ax2.set_ylim(-0.25, 0.25)\n",
    "ax2.set_xlabel('Elevation (m)')\n",
    "ax2.set_ylabel('Slope (degrees)')\n",
    "ax2.set_title('Elevation vs. Slope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plots for Elevation vs Distance to the Ocean Shore\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(14,7)) \n",
    "\n",
    "ax1.scatter(X[:,0],X[:,2], c=y, s=14, cmap=mpl.cm.Paired)\n",
    "ax1.set_xlabel('Elevation (m)')\n",
    "ax1.set_ylabel('Distance from the Ocean Shore (m)')\n",
    "ax1.set_title('Elevation vs. Distance to Shore')\n",
    "\n",
    "ax2.scatter(X[:,0],X[:,2], c=y, s=14, cmap=mpl.cm.Paired)\n",
    "ax2.set_xlim(-1.25, 0)\n",
    "ax2.set_ylim(-1.4, 0)\n",
    "ax2.set_xlabel('Elevation (m)')\n",
    "ax2.set_ylabel('Distance from the Ocean Shore (m)')\n",
    "ax2.set_title('Elevation vs. Distance to Shore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plots for Slope vs Distance to the Ocean Shore\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(14,7)) \n",
    "\n",
    "ax1.scatter(X[:,1],X[:,2], c=y, s=14, cmap=mpl.cm.Paired)\n",
    "ax1.set_xlabel('Slope (degrees)')\n",
    "ax1.set_ylabel('Distance from the Ocean Shore (m)')\n",
    "ax1.set_title('Slope vs. Distance to Shore')\n",
    "\n",
    "ax2.scatter(X[:,1],X[:,2], c=y, s=14, cmap=mpl.cm.Paired)\n",
    "ax2.set_xlim(-0.5, 14)\n",
    "ax2.set_ylim(-1.3, 0)\n",
    "ax2.set_xlabel('Slope (degrees)')\n",
    "ax2.set_ylabel('Distance from the Ocean Shore (m)')\n",
    "ax2.set_title('Slope vs. Distance to Shore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the training and testing data subsets**\n",
    "\n",
    "The training data is used to fit the Support Vector Machine (SVM) model (the SVM classifer is actually fit to the training data). We do initial testing and tuning of the model via cross-validation (coming next) and then \"test\" the model against the test data subset for generalization--its ability to predict using datasets of unknown classification.\n",
    "\n",
    "\n",
    "The holdout split of the test data is typically 20% (Aurelien, Geron, 2017. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O-Reilly Media, Inc. Per Andrew Ng (Stanford U. Machine Learning Course): training 60%, validation 20%, testing 20%. Since we're doing kfold cross-validation\n",
    "we don't need the validation set, so we'll stick with the 'Pareto' 80/20\n",
    "rule! However...cross-validation and optimization on approximately 8000 sample points is sure to chew up lots of computing resources (O(N^3) actually) and so require a lot of time (perhaps days) to process. To get around this issue to some degree we will split the training data (Xtrain) in half and do the cross-validation and optimization on those fewer points. Once optimized hyper-parameters are in hand we can then train the SVC using those parameters against the full training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sample Observations: 9787\n",
      "Observations in training set: 7829\n",
      "Observations in test set: 1958\n"
     ]
    }
   ],
   "source": [
    "holdout=0.2\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=holdout, random_state=42)\n",
    "\n",
    "print('Total Sample Observations:', len(X))\n",
    "print('Observations in training set:', len(Xtrain))\n",
    "print('Observations in test set:', len(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Validation and Parameter Tuning to Optimize the SVC**\n",
    "\n",
    "We use cross-validation in order to identify optimal values for the hyperparameters that will define our SVM. The parameters of interest are: kernel (linear, polynmial, or radial basis), gamma (if a radial basis function kernel proves best), degree (if a polynomial kernel is the best fit), and C the tolerance for data points falling inside the hyperplane margins.\n",
    "\n",
    "Note: some of the reporting code was copied, with modification, from the Sci-kit learn GridSearchCV documentation: http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7829 sample points for cross-validation\n",
      "Optimizing the SVC for precision\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.1, 'gamma': 200, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.841 (+/-0.098) for {'C': 0.1, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.877 (+/-0.018) for {'C': 0.1, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.908 (+/-0.018) for {'C': 0.1, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.926 (+/-0.005) for {'C': 0.1, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.935 (+/-0.007) for {'C': 0.1, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.944 (+/-0.014) for {'C': 0.1, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.916 (+/-0.008) for {'C': 0.1, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "0.877 (+/-0.019) for {'C': 1.0, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.903 (+/-0.009) for {'C': 1.0, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.926 (+/-0.007) for {'C': 1.0, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.935 (+/-0.016) for {'C': 1.0, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.939 (+/-0.011) for {'C': 1.0, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.944 (+/-0.011) for {'C': 1.0, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.942 (+/-0.015) for {'C': 1.0, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "0.898 (+/-0.011) for {'C': 10, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.916 (+/-0.008) for {'C': 10, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.936 (+/-0.011) for {'C': 10, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.936 (+/-0.013) for {'C': 10, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.939 (+/-0.009) for {'C': 10, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.941 (+/-0.012) for {'C': 10, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.928 (+/-0.007) for {'C': 10, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "0.910 (+/-0.014) for {'C': 100, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.929 (+/-0.008) for {'C': 100, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.937 (+/-0.014) for {'C': 100, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.937 (+/-0.013) for {'C': 100, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.938 (+/-0.010) for {'C': 100, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.931 (+/-0.009) for {'C': 100, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.929 (+/-0.007) for {'C': 100, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "0.913 (+/-0.016) for {'C': 1000, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.936 (+/-0.010) for {'C': 1000, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.932 (+/-0.020) for {'C': 1000, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.937 (+/-0.011) for {'C': 1000, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.936 (+/-0.008) for {'C': 1000, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.923 (+/-0.008) for {'C': 1000, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.928 (+/-0.007) for {'C': 1000, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.96      0.99      0.97      1568\n",
      "          1       0.94      0.84      0.89       390\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1958\n",
      "\n",
      "\n",
      "Optimizing the SVC for recall\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 20, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.511 (+/-0.004) for {'C': 0.1, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.918 (+/-0.012) for {'C': 0.1, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.929 (+/-0.019) for {'C': 0.1, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.942 (+/-0.009) for {'C': 0.1, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.937 (+/-0.017) for {'C': 0.1, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.895 (+/-0.021) for {'C': 0.1, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.666 (+/-0.022) for {'C': 0.1, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "0.918 (+/-0.012) for {'C': 1.0, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.920 (+/-0.016) for {'C': 1.0, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.943 (+/-0.014) for {'C': 1.0, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.946 (+/-0.018) for {'C': 1.0, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.948 (+/-0.015) for {'C': 1.0, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.945 (+/-0.016) for {'C': 1.0, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.876 (+/-0.015) for {'C': 1.0, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "0.915 (+/-0.013) for {'C': 10, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.929 (+/-0.020) for {'C': 10, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.946 (+/-0.014) for {'C': 10, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.947 (+/-0.018) for {'C': 10, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.952 (+/-0.016) for {'C': 10, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.946 (+/-0.014) for {'C': 10, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.880 (+/-0.005) for {'C': 10, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "0.915 (+/-0.018) for {'C': 100, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.943 (+/-0.017) for {'C': 100, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.946 (+/-0.013) for {'C': 100, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.948 (+/-0.017) for {'C': 100, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.952 (+/-0.019) for {'C': 100, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.011) for {'C': 100, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.877 (+/-0.010) for {'C': 100, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "0.929 (+/-0.021) for {'C': 1000, 'gamma': 0.002, 'kernel': 'rbf'}\n",
      "0.944 (+/-0.013) for {'C': 1000, 'gamma': 0.02, 'kernel': 'rbf'}\n",
      "0.947 (+/-0.013) for {'C': 1000, 'gamma': 0.2, 'kernel': 'rbf'}\n",
      "0.948 (+/-0.013) for {'C': 1000, 'gamma': 2, 'kernel': 'rbf'}\n",
      "0.948 (+/-0.017) for {'C': 1000, 'gamma': 20, 'kernel': 'rbf'}\n",
      "0.924 (+/-0.012) for {'C': 1000, 'gamma': 200, 'kernel': 'rbf'}\n",
      "0.877 (+/-0.007) for {'C': 1000, 'gamma': 2000, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.98      0.97      0.98      1568\n",
      "          1       0.88      0.94      0.91       390\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1958\n",
      "\n",
      "\n",
      "CPU times: user 10.3 s, sys: 761 ms, total: 11 s\n",
      "Wall time: 13min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Using', len(Xtrain), 'sample points for cross-validation')\n",
    "#parameters=[{'kernel':['rbf'], 'C':[1,10,100,1000], 'gamma':[7,8,9,10,11,12,14]}]\n",
    "\n",
    "# parameters for the initial \"wide-grid\" search:\n",
    "parameters=[{'kernel':['rbf'], 'C':[0.1,1.0,10,100,1000], 'gamma':[0.002,0.02,0.2,2,20,200,2000]}]\n",
    "\n",
    "#parameters=[{'kernel':['rbf'], 'C':[0.1,1.0,10,100,1000], 'gamma':[0.002,0.02,0.2,2,20]}]\n",
    "\n",
    "\n",
    "scores=['precision','recall']\n",
    "for score in scores:\n",
    "    print('Optimizing the SVC for %s' % score)\n",
    "    cv = GridSearchCV(estimator=svm.SVC(C=1),param_grid=parameters, cv=5, n_jobs=-1, scoring='%s_macro' % score)\n",
    "    cv.fit(Xtrain,ytrain)\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(cv.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = cv.cv_results_['mean_test_score']\n",
    "    stds = cv.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, cv.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = ytest, cv.predict(Xtest)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Confusion Matrix**\n",
    "\n",
    "Display a count of the number of observations correctly classified and number nor correctly classified for each class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a confusion matrix to get a count of the number of observations correctly classified\n",
    "# and the number NOT correctly classified:\n",
    "#print(confusion_matrix(ytest,sv.predict(Xtest) ))\n",
    "print('Confusion Matrix:')\n",
    "print(pd.DataFrame(confusion_matrix(y_true,y_pred), index=[-1,1], columns=[-1,1]) )\n",
    "print()\n",
    "print(cv.best_estimator_.score(Xtest, ytest).round(3))\n",
    "print()\n",
    "print('Geometric Mean:' )\n",
    "print( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate (learn) an SVM model**\n",
    "\n",
    "Using the best estimators from the GridSearch cross-validation build a new (optimized) SVC with the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sv=svm.SVC(C=100, kernel='rbf', gamma=12).fit(Xtrain,ytrain)\n",
    "print('Number of support vectors (class -1 class 1):', sv.n_support_)\n",
    "print('Model score:', sv.score(Xtrain,ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify (predict) the U.S. Atlantic Coast test data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = sv.predict(Xtest)\n",
    "print(y_pred)\n",
    "np.shape(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the model against a fresh data set**\n",
    "\n",
    "We generated another 7000 point random sample across the study area in GRASS in order to further test the efficacy of the new sv SVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the new sample point set from GRASS, selecting only those records\n",
    "# whose elevation value (value) is >= 0 meters:\n",
    "\n",
    "df7 = pd.read_sql_query('select * from test_7k_pts where value >= 0', conn)\n",
    "\n",
    "# scale the data to standard normal with mean=0 and standard deviation=1:\n",
    "X7 = scaler.fit_transform(np.array(df7[['value','slope','distance']]))\n",
    "print('Fitting', len(X7), 'points to sv')\n",
    "y7 = sv.predict(X7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y7)\n",
    "print(np.shape(df1), np.shape(X7), np.shape(y7))\n",
    "df_ = df7.join(pd.DataFrame(y7, columns=['label']))\n",
    "df7\n",
    "df_.to_csv('/Users/paulp/GoogleDrive/projects/CoastalPlains/data/fit_7k_pts', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the model in a location outside the U.S. Atlantic Coast domain**\n",
    "\n",
    "We've downloaded a small section of the South American Coast in the northeast across the Guyana and Suriname Republics. These are areas where there is a well-defined, albeit small, coastal plain bounded by mesozoic and older rock bodies. We'll run a set of 5000 sample points thru the sv SVC to see how well a model, contoured to fit the geomorphology of the east coast of North America generalizes to other areas. Ultimately, in order to present a more globally generalized model--that is, one that 'best-fits' coastal regions and coastal plains (where they exist) around the world. Tall order perhaps, indeed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the new sample point set from GRASS, selecting only those records\n",
    "# whose elevation value (value) is >= 0 meters:\n",
    "\n",
    "dfbc = pd.read_sql_query('select * from Guyanas_5kpts where value >= 0', conn).dropna()\n",
    "\n",
    "loc = np.array(dfbc[['cat','x','y']])\n",
    "# scale the data to standard normal with mean=0 and standard deviation=1:\n",
    "X = scaler.fit_transform(np.array(dfbc[['elevation','slope','distance']]))\n",
    "print('Fitting', len(X), 'points to sv')\n",
    "y = sv.predict(X)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(loc))\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfg1 = pd.DataFrame(loc, columns=['cat','x','y'])\n",
    "dfg2 = pd.DataFrame(X, columns=['elevation','slope','distance'])\n",
    "dfg3 = pd.DataFrame(y, columns=['label'])\n",
    "\n",
    "dfg = pd.concat([dfg1,dfg2,dfg3], axis=1)\n",
    "dfg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfg.to_csv('/Users/paulp/GoogleDrive/projects/CoastalPlains/data/guyana_5k_pts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for the Guyanas data were plotted in QGIS. Upon inspection it was immediately clear that the North American-based SVM was far from optimal for the northeastern corner of South America. The problem, I'm quite certain, is due in the largest part to model overfitting. I've 'tuned' the N. American SVM to closely to the nuances of that region's topography such that the model, in its present form, generalizes poorly. This was, believe it or not, not unexpected. \n",
    "\n",
    "The fix is to 'detune' the model for N. America and retune using more sophisticated optimizations methods, coupled with training data taken from a broader geographic region. In the future, we will experiment with fitting a new SVM classifier using:\n",
    "\n",
    "-  gradient descent and/or\n",
    "-  stochastic gradient descent \n",
    "\n",
    "to isolate the optimal hyper-parameters and learning rates, all with the goal of crafting a coastal model that 'learns on the fly'. That is, as new data are added for model training, the model can, without rebuilding, adapt to new data, and hence new and differing topographic characteristics for the world's coastal regions. In the end we'll have a dynamic, adptive learning model that can discriminate and map the coastal plain regions, and ultimately, the entirety of the continental margin, of which, coastal plains are an integral component. \n",
    "\n",
    "Borrowing from a professor of old, Dr. R. K. Bambach: 'The modeling ain't done yet!'\n",
    "\n",
    "Stay tuned..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
